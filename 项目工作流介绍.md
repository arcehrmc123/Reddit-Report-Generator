# Reddit Report Generator - 项目工作流介绍

## 🚀 项目概述

Reddit Report Generator 是一个基于区块链分析架构 TIM 项目的社交媒体分析系统。它使用多智能体方法和大语言模型 (LLM) 来提供对 Reddit 用户和社区的全面分析。

## 📊 数据来源

系统使用两个主要的 Reddit 数据集：
- `r_OpenAI_posts.jsonl`: 包含 r/OpenAI 子版的 **111,461** 个帖子
- `r_OpenAI_comments.jsonl`: 包含 r/OpenAI 子版的 **1,491,460** 条评论

## 🎯 核心功能

### 1. 数据加载与验证
- JSONL 文件读取器，具有适当的错误处理
- 数据验证和清理工具
- 支持大型数据集（超过 2.5 GB）

### 2. 核心分析功能
- **用户活动统计**：计算帖子、评论和总活动数
- **Karma 分析**：计算帖子 karma、评论 karma 和总 karma
- **内容分析**：提取关键词并分析情感
- **社区分析**：识别顶级作者、活动模式和参与度指标

### 3. 多智能体分析架构
遵循 TIM 项目的设计，我们实现了：
- **MetaController**：规划分析流程并协调多个视角
- **DomainExpertAnalyst**：从特定领域视角进行分析
- **QuestionSolverAnalyst**：收集数据并回答特定问题
- **StatelessChecker**：验证分析结果的一致性
- **StatelessScorer**：汇总并评分所有分析结果

### 4. 工具与实用程序
- **Reddit 工具**：从 Reddit 数据集中提取和分析数据
- **网页抓取工具**：从外部来源搜索和提取信息
- **情感分析**：文本情感分析（为真实 NLP 模型预留位置）
- **关键词提取**：简单的基于频率的关键词提取

## 🔄 工作流程详细介绍

### 1. 初始化阶段
```
1. 加载环境变量和配置
2. 初始化 OpenAI 客户端（使用提供的 API 密钥）
3. 加载 Reddit 数据集（帖子和评论）
4. 如果是用户分析，收集该用户的所有活动
```

### 2. 元规划阶段 (MetaController)
**Prompt:**
```
ROLE: 你是 Reddit 用户/社区分析的元控制器。你负责构建分析流程并协调分析视角。

ACTION: 为了分析 Reddit 用户/社区 {user_or_community_id}，我们组建了一个由多位领域专家分析师组成的团队。
基于你的专业知识，请为每位分析师设计一个结构化的计划。该计划应包括多个视角，每个视角将自动分配给特定的智能体。
每个任务应包含详细的提示，以有效地指导分析师。

建议的考虑因素：
- 内容分析：分析帖子和评论的文本内容，包括讨论的主题、情感、语言模式和内容质量。寻找重复出现的主题、关键兴趣和交流风格。
- 用户行为：分析用户的活动模式，包括发帖频率、评论频率、参与度（点赞、评论、分享）以及与其他用户的互动模式。
- 社区参与：检查用户在社区中的角色和影响，包括他们的影响力、声誉以及对讨论的贡献。
- 网络分析：分析用户的互动网络，识别关键连接、影响模式以及子版内的社区结构。
- 其他视角：如果你有其他视角想法，请添加到计划中。

已知工具：
{工具列表}
```

**输出格式要求：**
```
必须返回 JSON 格式，包含以下字段：
- perspectives: 分析视角列表，每个视角包括：
  - name: 视角名称
  - description: 视角描述
  - prompt: 视角分析的提示
  - tool_suggestions: 工具建议列表
  - tips: 分析提示
```

### 3. 并行分析阶段 (DomainExpertAnalyst + QuestionSolverAnalyst)

#### DomainExpertAnalyst - 领域专家
**Prompt:**
```
ROLE: 你是 {perspective} 领域的专业 Reddit 用户/社区分析师。下面我会向你提出一个请求。记住，你在琐事方面是 Ken Jennings 级别的，在解决谜题方面是门萨级别的，所以你的知识储备应该非常丰富。
ACTION: 从 {perspective} 视角分析 Reddit 用户/社区，以发现见解和模式。

请分析 {self.plan.target}。

这里有一个分析 {self.plan.target} 的计划：
{任务列表}

现在请根据收集到的数据进行分析。
```

#### QuestionSolverAnalyst - 问题解决者
**Prompt:**
```
ROLE: 你是 {perspective} 领域的专业 Reddit 用户/社区分析师，擅长解决问题。下面我会向你提出一个请求。记住，你在琐事方面是 Ken Jennings 级别的，在解决谜题方面是门萨级别的，所以你的知识储备应该非常丰富。
ACTION: 从 {perspective} 视角为主要分析师收集尽可能多的信息，直到请求完全满足。在分析中提供完整的信息和结论。不要提问或请求进一步的指示 - 只需根据可用信息提供最佳的完整分析。请求时不要使用任何占位符。当你完全解决了问题，请说 END。

已知事实：
{known_facts}

需要分析的问题：
{question}

{prompt}
```

### 4. 验证阶段 (StatelessChecker)
**Prompt:**
```
ROLE: Reddit 分析报告评估器

你是一位专家评估员，负责检查分析结果的逻辑一致性和推理链。你需要逐步评估提供的分析，并识别任何矛盾或缺失的逻辑链接。你应该推断每个提供的分析报告的整体质量，并根据证据质量和推理可靠性分配可信度权重。

下面是对同一 Reddit 用户/社区的不同视角分析报告：
{analysis}

分析此内容，确定每个视角的可信度权重，并识别分析的整体质量。
根据以下内容为你的可信度评估提供理由：
- 证据质量和相关性
- 推理的合理性和逻辑一致性
- 分析的完整性
- 推测与事实推理的存在
- 视角之间的一致性
- 数据源的质量

分析类别：
{analysis_categories}

响应必须遵循以下 JSON 模式：
{check_report_schema}
```

### 5. 评分与报告生成阶段 (StatelessScorer)
**Prompt:**
```
ROLE: 你是最终评估者，负责确定 Reddit 用户/社区的综合分析。

下面是对同一 Reddit 用户/社区的不同视角分析报告：
{analysis}

以下是每个视角的意图和可信度的加权评估：
{check_report}

请按以下步骤进行评估：
1. 评估分析结果中推理链的逻辑一致性。
2. 识别任何矛盾或缺失的逻辑链接。
3. 解释任何检测到的不一致性背后的推理。
4. 将所有视角综合成一份全面的最终分析。
5. 从分析中识别关键见解。
6. 突出优势和劣势/改进领域。
7. 提供可操作的建议。
8. 计算整体置信度分数。

{final_report_schema}
```

## 🚀 使用方法

### 1. 列出顶级作者
```bash
cd D:\项目\reddit_report
python -m poetry run python -m RedditReportGenerator list-authors --limit 5
```

**输出示例：**
```
Top 5 authors in the dataset:
1. [deleted]: 149560 activities
2. AutoModerator: 24907 activities
3. sdmat: 2451 activities
4. traumfisch: 1934 activities
5. Xtianus21: 1482 activities
```

### 2. 分析单个用户
```bash
python -m poetry run python -m RedditReportGenerator analyze --user "sdmat" --openai-api-key "sk-your-key-here"
```

### 3. 从配置文件开始分析
```bash
python -m poetry run python -m RedditReportGenerator start --config config.json --openai-api-key "sk-your-key-here"
```

### 4. 启动 Gradio 演示
```bash
python -m poetry run python -m RedditReportGenerator demo --openai-api-key "sk-your-key-here"
```

### 5. 启动 FastAPI 服务器
```bash
python -m poetry run python -m RedditReportGenerator serve --openai-api-key "sk-your-key-here"
```

## 📈 分析示例

### 用户分析示例 (sdmat - r/OpenAI 顶级作者)
- **总活动**: 2451 (26 个帖子，2425 条评论)
- **Karma**: 14166 (1701 个帖子 karma，12465 条评论 karma)

### 社区分析指标
- **总帖子数**: 111,461
- **总评论数**: 1,491,460
- **活跃作者**: 数千名独特作者

## 🔧 技术实现

### 核心技术
- **Python**: 主要编程语言
- **Poetry**: 依赖管理
- **OpenAI API**: LLM 集成（兼容 Groq、Google AI 等）
- **LangChain**: 智能体编排
- **FastAPI/Gradio**: Web 界面

### 模型配置
- 默认 LLM: gpt-4o-mini
- 令牌限制: 128,000
- 温度: 0.7（用于创造性任务），0（用于确定性任务）

### 安装与设置
1. 安装依赖: `poetry install`
2. 在 `.env` 文件中设置 API 密钥:
   ```
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_BASE_URL=https://api.openai.com/v1
   ```

## 📝 输出结果

分析结果保存在以下位置：
- `meta_plans/`: 每个用户/社区的分析计划
- `check_reports/`: 每个用户/社区的验证报告
- `score_reports/`: 最终分析报告（Markdown 格式）

## 🏆 项目成就

1. 成功实现了 TIM 项目的架构用于 Reddit 分析
2. 创建了一个具有所有核心功能的完整系统
3. 高效处理了非常大的数据集
4. 实现了全面的错误处理和日志记录
5. 测试并验证了所有关键功能

## 📞 下一步操作

1. **获取 API 密钥**: 在 https://platform.openai.com/account/api-keys 注册获取 OpenAI API 密钥
2. **配置系统**: 在 `.env` 文件中设置 API 密钥
3. **首次分析**: 使用 `python -m poetry run python -m RedditReportGenerator list-authors --limit 5` 进行测试
4. **分析目标用户**: 使用 `analyze` 命令获取特定用户的详细报告
5. **自定义配置**: 根据需要修改 `config.json` 和 `analysis_categories.json`

该系统现已完全功能正常，并且已准备好使用有效的 OpenAI API 密钥进行分析！
